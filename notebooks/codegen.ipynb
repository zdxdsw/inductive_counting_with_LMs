{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingshan/llms_do_math/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json, random, os, io, pickle\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using unk_token, but it is not set yet.\n",
      "Using unk_token, but it is not set yet.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.99s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(51200, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen25-7b-instruct\", torch_dtype=torch.float16, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen25-7b-instruct\", torch_dtype=torch.float16)\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# this script solves the following math problem and prints the answer.\n",
      "# problem: What is the second largest prime factor of 183?\n",
      "var_a = 183\n",
      "var_b = 2\n",
      "var_c = 0\n",
      "var_d = 0\n",
      "var_e = 0\n",
      "var_f = 0\n",
      "var_g = 0\n",
      "var_h = 0\n",
      "var_i = 0\n",
      "var_j = 0\n",
      "var_k = 0\n",
      "var_l = 0\n",
      "var_m = 0\n",
      "var_n = 0\n",
      "var_o = 0\n",
      "var_p = 0\n",
      "var_q = 0\n",
      "var_r = 0\n",
      "var_s = 0\n",
      "var_t = 0\n",
      "var_u = 0\n",
      "var_v = 0\n",
      "var_w = 0\n",
      "var_x = 0\n",
      "var_y = 0\n",
      "var_z = 0\n",
      "var_aa = 0\n",
      "var_ab = 0\n",
      "var_ac = 0\n",
      "var_ad = 0\n",
      "var_ae = 0\n",
      "var_af = 0\n",
      "var_ag = 0\n",
      "var_ah = 0\n",
      "var_ai = 0\n",
      "var_aj = 0\n",
      "var_ak = 0\n",
      "var_al = 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prefix = (\n",
    "          \"# this script solves the following math problem and prints the answer.\\n\"\n",
    "          \"# problem: What is the second largest prime factor of 183?\\n\"\n",
    "          #\"def func(\"\n",
    "          \"var_a =\"\n",
    "          )\n",
    "input_ids = tokenizer(prefix, return_tensors=\"pt\").input_ids.to(0)\n",
    "generated_ids = model.generate(input_ids, max_length=256)\n",
    "output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)#[len(prefix)-7:]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def func(n):\n",
    "    if n == 1:\n",
    "        return False\n",
    "    for i in range(2, int(n ** 0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def prime_factors(n):\n",
    "    factors = []\n",
    "    while n % 2 == 0:\n",
    "        factors.append(2)\n",
    "        n //= 2\n",
    "    for i in range(3, int(n ** 0.5) + 1, 2):\n",
    "        while n % i == 0:\n",
    "            factors.append(i)\n",
    "            n //= i\n",
    "    if n > 2:\n",
    "        factors.append(n)\n",
    "    return factors\n",
    "\n",
    "def second_largest_prime_factor(n):\n",
    "    factors = prime_factors(n)\n",
    "    factors.sort()\n",
    "    return factors[-2]\n",
    "\n",
    "\n",
    "print(second_largest_prime_factor(183))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "183 // 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
