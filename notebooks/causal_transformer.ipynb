{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, math, sys, random, re, pytz\n",
    "from datetime import datetime\n",
    "timezone = pytz.timezone('America/New_York') \n",
    "import torch\n",
    "sys.path.append(\"../scripts/\")\n",
    "from causal_transformer.model import Causal_Transformer\n",
    "from causal_transformer.config import *\n",
    "from causal_transformer.dataset import sequences_collator\n",
    "from causal_transformer.utils import get_acc\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import concatenate_datasets\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from collections import defaultdict, Counter\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "task = \"counting_raspL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = eval(f\"{task}_Config()\")\n",
    "\n",
    "ckpt_dir = \"/data/yingshac/llms_do_math/scripts/causal_transformer/output\"\n",
    "load_from_dir = \"0411_232651\"\n",
    "load_from_config = json.load(open(os.path.join(\"../scripts/causal_transformer/output\", load_from_dir, \"config.json\"), \"r\"))\n",
    "for k in load_from_config:\n",
    "    setattr(config, k, load_from_config[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Causal_Transformer(config)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from 4_39065_transformer.pt\n"
     ]
    }
   ],
   "source": [
    "load_from_specific_epc = 5\n",
    "ckpt_dir = os.path.join(ckpt_dir, load_from_dir, \"ckpts\")\n",
    "if load_from_specific_epc is None:\n",
    "    load_from_pt = sorted(os.listdir(ckpt_dir), key=lambda x: int(x.split(\"_\")[1]))[-1]\n",
    "else:\n",
    "    load_from_pt = sorted([x for x in os.listdir(ckpt_dir) if f\"{load_from_specific_epc-1}_\" in x[:4]], key=lambda x: int(x.split(\"_\")[1]))[-1]\n",
    "state_dict = torch.load(os.path.join(ckpt_dir, load_from_pt), map_location=device)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "print(f\"load from {load_from_pt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f\"../data/rasp_primitives/{task}\"\n",
    "split = \"ood_test\"\n",
    "test_data = load_dataset(\n",
    "                    \"text\", \n",
    "                    data_files={split: f\"{data_path}/{split}.txt\"})\n",
    "\n",
    "collator = partial(sequences_collator, \n",
    "                   w2i={w:i for i,w in enumerate(config.vocab)}, \n",
    "                   max_len=config.max_position_embeddings,\n",
    "                   posemb_shift=config.absolute_posemb_shift or config.rotary_posemb_shift\n",
    "                   )\n",
    "test_dataloader = DataLoader(test_data[split], shuffle=False, batch_size=config.per_device_train_batch_size, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        | Test Loss: 0.2256 \n",
      "        | Test Acc: 0.9685 \n",
      "        | Test Counting Acc: 0.9685 \n",
      "        | Test Last Acc: 0.9704\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "counting_correct, counting_demo, last_correct, last_demo, correct, demo = 0, 0, 0, 0, 0, 0\n",
    "test_losses = []\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "model.eval()\n",
    "testing_output = {}\n",
    "\n",
    "date = datetime.now(timezone).strftime(\"%m%d_%H%M%S\")\n",
    "\n",
    "k = 0\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    position_ids = None\n",
    "    if batch['position_id'] is not None: position_ids = batch['position_id'].to(device)\n",
    "    \n",
    "    logits = model(\n",
    "        batch['input_id'].to(device),\n",
    "        position_ids = position_ids,\n",
    "        attention_mask = batch['attention_mask'].to(device),\n",
    "    )\n",
    "\n",
    "    loss = criterion(\n",
    "        logits.view(-1, logits.size(-1)), # bs*seq_len, vocab_size\n",
    "        batch['label'].view(-1).to(device), # 1, bs*seq_len\n",
    "    )\n",
    "    test_losses.append(loss.detach().item())\n",
    "    _counting_correct, _counting_demo, _last_correct, _last_demo = get_acc(logits.detach().cpu(), batch['label'].detach().cpu(), ignore_index=-1)\n",
    "    counting_correct += _counting_correct\n",
    "    counting_demo += _counting_demo\n",
    "    last_correct += _last_correct\n",
    "    last_demo += _last_demo\n",
    "    correct += (_counting_correct + _last_correct)\n",
    "    demo += (_counting_demo + _last_demo)\n",
    "   \n",
    "    for input_id, gth_id, pred_id in zip(batch['input_id'], batch['label'], logits.argmax(dim=-1)):\n",
    "        input_seq = [config.vocab[i] for i in input_id if config.vocab[i]!='<pad>']\n",
    "        gth_seq = [config.vocab[gth_id[i]] for i in range(len(gth_id)) if gth_id[i]!=-1]\n",
    "        pred_seq = [config.vocab[pred_id[i]] for i in range(len(gth_id)) if gth_id[i]!=-1][:len(gth_seq)]\n",
    "        testing_output[k] = {\n",
    "            \"input\": \" \".join(input_seq),\n",
    "            \"gth\": \" \".join(gth_seq),\n",
    "            \"pred\": \" \".join(pred_seq),\n",
    "        }\n",
    "        k+=1\n",
    "    \n",
    "print(f\"\"\"\n",
    "        | Test Loss: {round(np.mean(test_losses), 4)} \n",
    "        | Test Acc: {round(correct/demo, 4)} \n",
    "        | Test Counting Acc: {round(counting_correct/counting_demo, 4)} \n",
    "        | Test Last Acc: {round(last_correct/last_demo, 4)}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"../scripts/causal_transformer/output/{load_from_dir}/test_samples\", exist_ok=True)\n",
    "json.dump({\n",
    "        \"test_data_file\":  f\"{data_path}/{split}.txt\",\n",
    "        \"load_from\": f\"{load_from_dir}/{load_from_pt}\",\n",
    "        \"test_acc\": round(correct/demo, 4),\n",
    "        \"test_counting_acc\": round(counting_correct/counting_demo, 4),\n",
    "        \"test_last_acc\": round(last_correct/last_demo, 4),\n",
    "        \"test_loss\": round(np.mean(test_losses), 4),\n",
    "        \"testing_output\": testing_output,\n",
    "    }, \n",
    "    open(f\"../scripts/causal_transformer/output/{load_from_dir}/test_samples/{date}.json\", \"w\"), indent=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+---------+--------------+\n",
      "|        Modules         | #Params | Param shape  |\n",
      "+------------------------+---------+--------------+\n",
      "|       wte.weight       |  131072 | [128, 1024]  |\n",
      "|       wpe.weight       |  131072 | [128, 1024]  |\n",
      "|    h.0.ln_1.weight     |   1024  |    [1024]    |\n",
      "|     h.0.ln_1.bias      |   1024  |    [1024]    |\n",
      "| h.0.attn.c_attn.weight | 3145728 | [1024, 3072] |\n",
      "|  h.0.attn.c_attn.bias  |   3072  |    [3072]    |\n",
      "| h.0.attn.c_proj.weight | 1048576 | [1024, 1024] |\n",
      "|  h.0.attn.c_proj.bias  |   1024  |    [1024]    |\n",
      "|    h.0.ln_2.weight     |   1024  |    [1024]    |\n",
      "|     h.0.ln_2.bias      |   1024  |    [1024]    |\n",
      "|  h.0.mlp.c_fc.weight   | 4194304 | [1024, 4096] |\n",
      "|   h.0.mlp.c_fc.bias    |   4096  |    [4096]    |\n",
      "| h.0.mlp.c_proj.weight  | 4194304 | [4096, 1024] |\n",
      "|  h.0.mlp.c_proj.bias   |   1024  |    [1024]    |\n",
      "|    h.1.ln_1.weight     |   1024  |    [1024]    |\n",
      "|     h.1.ln_1.bias      |   1024  |    [1024]    |\n",
      "| h.1.attn.c_attn.weight | 3145728 | [1024, 3072] |\n",
      "|  h.1.attn.c_attn.bias  |   3072  |    [3072]    |\n",
      "| h.1.attn.c_proj.weight | 1048576 | [1024, 1024] |\n",
      "|  h.1.attn.c_proj.bias  |   1024  |    [1024]    |\n",
      "|    h.1.ln_2.weight     |   1024  |    [1024]    |\n",
      "|     h.1.ln_2.bias      |   1024  |    [1024]    |\n",
      "|  h.1.mlp.c_fc.weight   | 4194304 | [1024, 4096] |\n",
      "|   h.1.mlp.c_fc.bias    |   4096  |    [4096]    |\n",
      "| h.1.mlp.c_proj.weight  | 4194304 | [4096, 1024] |\n",
      "|  h.1.mlp.c_proj.bias   |   1024  |    [1024]    |\n",
      "|    h.2.ln_1.weight     |   1024  |    [1024]    |\n",
      "|     h.2.ln_1.bias      |   1024  |    [1024]    |\n",
      "| h.2.attn.c_attn.weight | 3145728 | [1024, 3072] |\n",
      "|  h.2.attn.c_attn.bias  |   3072  |    [3072]    |\n",
      "| h.2.attn.c_proj.weight | 1048576 | [1024, 1024] |\n",
      "|  h.2.attn.c_proj.bias  |   1024  |    [1024]    |\n",
      "|    h.2.ln_2.weight     |   1024  |    [1024]    |\n",
      "|     h.2.ln_2.bias      |   1024  |    [1024]    |\n",
      "|  h.2.mlp.c_fc.weight   | 4194304 | [1024, 4096] |\n",
      "|   h.2.mlp.c_fc.bias    |   4096  |    [4096]    |\n",
      "| h.2.mlp.c_proj.weight  | 4194304 | [4096, 1024] |\n",
      "|  h.2.mlp.c_proj.bias   |   1024  |    [1024]    |\n",
      "|    h.3.ln_1.weight     |   1024  |    [1024]    |\n",
      "|     h.3.ln_1.bias      |   1024  |    [1024]    |\n",
      "| h.3.attn.c_attn.weight | 3145728 | [1024, 3072] |\n",
      "|  h.3.attn.c_attn.bias  |   3072  |    [3072]    |\n",
      "| h.3.attn.c_proj.weight | 1048576 | [1024, 1024] |\n",
      "|  h.3.attn.c_proj.bias  |   1024  |    [1024]    |\n",
      "|    h.3.ln_2.weight     |   1024  |    [1024]    |\n",
      "|     h.3.ln_2.bias      |   1024  |    [1024]    |\n",
      "|  h.3.mlp.c_fc.weight   | 4194304 | [1024, 4096] |\n",
      "|   h.3.mlp.c_fc.bias    |   4096  |    [4096]    |\n",
      "| h.3.mlp.c_proj.weight  | 4194304 | [4096, 1024] |\n",
      "|  h.3.mlp.c_proj.bias   |   1024  |    [1024]    |\n",
      "|      ln_f.weight       |   1024  |    [1024]    |\n",
      "|       ln_f.bias        |   1024  |    [1024]    |\n",
      "|     lm_head.weight     |  131072 | [128, 1024]  |\n",
      "+------------------------+---------+--------------+\n",
      "Total Params: 50780160\n"
     ]
    }
   ],
   "source": [
    "from causal_transformer.utils import count_parameters\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1, 1, 1, 1, 1, 1, 0, 1],\n",
      "          [1, 1, 1, 1, 0, 0, 0, 1],\n",
      "          [0, 1, 1, 0, 0, 0, 1, 1],\n",
      "          [0, 0, 1, 1, 1, 0, 0, 1],\n",
      "          [1, 1, 0, 0, 0, 1, 0, 1],\n",
      "          [0, 0, 0, 0, 1, 1, 0, 0],\n",
      "          [1, 0, 0, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 1, 0, 1, 1, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 0, 0, 1, 1, 1, 0, 1],\n",
      "          [1, 1, 1, 1, 1, 1, 1, 0],\n",
      "          [0, 1, 1, 0, 0, 0, 1, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0, 1],\n",
      "          [0, 0, 0, 1, 0, 1, 1, 0],\n",
      "          [0, 0, 0, 0, 1, 0, 1, 0],\n",
      "          [1, 0, 0, 1, 0, 0, 0, 1]]],\n",
      "\n",
      "\n",
      "        [[[1, 1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 0, 0, 1, 0, 0, 0, 0],\n",
      "          [0, 0, 0, 1, 0, 0, 0, 0],\n",
      "          [1, 0, 0, 0, 1, 0, 1, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0, 1],\n",
      "          [1, 0, 0, 1, 0, 0, 1, 0],\n",
      "          [1, 0, 1, 0, 0, 1, 1, 1],\n",
      "          [1, 1, 1, 0, 1, 0, 1, 1]]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(0, 2, (3, 1, 8, 8))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8, 8])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eye = torch.eye(8).unsqueeze(0)[:, None, :, :]\n",
    "eye.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 1, 1, 1, 1, 1, 0, 1],\n",
       "          [1, 1, 1, 1, 0, 0, 0, 1],\n",
       "          [0, 1, 1, 0, 0, 0, 1, 1],\n",
       "          [0, 0, 1, 1, 1, 0, 0, 1],\n",
       "          [1, 1, 0, 0, 1, 1, 0, 1],\n",
       "          [0, 0, 0, 0, 1, 1, 0, 0],\n",
       "          [1, 0, 0, 1, 1, 0, 1, 0],\n",
       "          [1, 1, 0, 1, 0, 1, 1, 1]]],\n",
       "\n",
       "\n",
       "        [[[1, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 1, 1, 1, 0, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 0],\n",
       "          [0, 1, 1, 1, 0, 0, 1, 0],\n",
       "          [1, 1, 1, 1, 1, 0, 0, 1],\n",
       "          [0, 0, 0, 1, 0, 1, 1, 0],\n",
       "          [0, 0, 0, 0, 1, 0, 1, 0],\n",
       "          [1, 0, 0, 1, 0, 0, 0, 1]]],\n",
       "\n",
       "\n",
       "        [[[1, 1, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 0, 1, 0, 0, 0, 0],\n",
       "          [0, 0, 1, 1, 0, 0, 0, 0],\n",
       "          [1, 0, 0, 1, 1, 0, 1, 0],\n",
       "          [1, 1, 1, 1, 1, 0, 0, 1],\n",
       "          [1, 0, 0, 1, 0, 1, 1, 0],\n",
       "          [1, 0, 1, 0, 0, 1, 1, 1],\n",
       "          [1, 1, 1, 0, 1, 0, 1, 1]]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x.bool() | eye.bool()).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_do_math",
   "language": "python",
   "name": "llms_do_math"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
