{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingshac/workspace/llms_do_math/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json, os, math, sys, random, re, pytz\n",
    "from datetime import datetime\n",
    "timezone = pytz.timezone('America/New_York') \n",
    "import torch\n",
    "sys.path.append(\"../scripts/\")\n",
    "from causal_transformer.model import Causal_Transformer\n",
    "from causal_transformer.config import *\n",
    "from causal_transformer.dataset import sequences_collator\n",
    "from causal_transformer.utils import get_acc\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import concatenate_datasets\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from collections import defaultdict, Counter\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from 2_93750_transformer.pt\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "task = \"counting_diffsymbol_mod16\"\n",
    "config = eval(f\"{task}_Config()\")\n",
    "ckpt_dir = \"/data/yingshac/llms_do_math/scripts/causal_transformer/output\"\n",
    "\n",
    "load_from_dir = \"0416_093958\"\n",
    "load_from_specific_epc = 3\n",
    "\n",
    "load_from_config = json.load(open(os.path.join(\"../scripts/causal_transformer/output\", load_from_dir, \"config.json\"), \"r\"))\n",
    "for k in load_from_config:\n",
    "    setattr(config, k, load_from_config[k])\n",
    "model = Causal_Transformer(config)\n",
    "model = model.to(device)\n",
    "\n",
    "ckpt_dir = os.path.join(ckpt_dir, load_from_dir, \"ckpts\")\n",
    "if load_from_specific_epc is None:\n",
    "    load_from_pt = sorted(os.listdir(ckpt_dir), key=lambda x: int(x.split(\"_\")[1]))[-1]\n",
    "else:\n",
    "    load_from_pt = sorted([x for x in os.listdir(ckpt_dir) if f\"{load_from_specific_epc-1}_\" in x[:4]], key=lambda x: int(x.split(\"_\")[1]))[-1]\n",
    "state_dict = torch.load(os.path.join(ckpt_dir, load_from_pt), map_location=device)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "print(f\"load from {load_from_pt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num ood_test data = 2800\n"
     ]
    }
   ],
   "source": [
    "data_path = f\"../data/rasp_primitives/{task}\"\n",
    "split = \"ood_test\"\n",
    "test_data = load_dataset(\n",
    "                    \"text\", \n",
    "                    data_files={split: f\"{data_path}/{split}.txt\"})\n",
    "print(f\"num {split} data = {len(test_data[split])}\")\n",
    "\n",
    "if config.absolute_posemb_shift or config.rotary_posemb_shift:\n",
    "    augmentation = \"shift\"\n",
    "elif config.absolute_posemb_rdmz or config.rotary_posemb_rdmz:\n",
    "    augmentation = \"randomized\"\n",
    "collator = partial(sequences_collator, \n",
    "                   w2i={w:i for i,w in enumerate(config.vocab)}, \n",
    "                   max_len=config.max_position_embeddings,\n",
    "                   augmentation=augmentation,\n",
    "                   )\n",
    "test_dataloader = DataLoader(test_data[split], shuffle=False, batch_size=config.per_device_train_batch_size, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ood_test acc\n",
      "        | Test Loss: 0.0003 \n",
      "        | Test Acc: 0.9999 \n",
      "        | Test Counting Acc: 0.9999 \n",
      "        | Test Last Acc: 0.9989\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "counting_correct, counting_demo, last_correct, last_demo, correct, demo = 0, 0, 0, 0, 0, 0\n",
    "test_losses = []\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "model.eval()\n",
    "testing_output = {}\n",
    "\n",
    "date = datetime.now(timezone).strftime(\"%m%d_%H%M%S\")\n",
    "\n",
    "k = 0\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    position_ids = None\n",
    "    if batch['position_id'] is not None: position_ids = batch['position_id'].to(device)\n",
    "    \n",
    "    logits = model(\n",
    "        batch['input_id'].to(device),\n",
    "        position_ids = position_ids,\n",
    "        attention_mask = batch['attention_mask'].to(device),\n",
    "    )\n",
    "\n",
    "    loss = criterion(\n",
    "        logits.view(-1, logits.size(-1)), # bs*seq_len, vocab_size\n",
    "        batch['label'].view(-1).to(device), # 1, bs*seq_len\n",
    "    )\n",
    "    test_losses.append(loss.detach().item())\n",
    "    _counting_correct, _counting_demo, _last_correct, _last_demo = get_acc(logits.detach().cpu(), batch['label'].detach().cpu(), ignore_index=-1)\n",
    "    counting_correct += _counting_correct\n",
    "    counting_demo += _counting_demo\n",
    "    last_correct += _last_correct\n",
    "    last_demo += _last_demo\n",
    "    correct += (_counting_correct + _last_correct)\n",
    "    demo += (_counting_demo + _last_demo)\n",
    "   \n",
    "    for input_id, gth_id, pred_id in zip(batch['input_id'], batch['label'], logits.argmax(dim=-1)):\n",
    "        input_seq = [config.vocab[i] for i in input_id if config.vocab[i]!='<pad>']\n",
    "        gth_seq = [config.vocab[gth_id[i]] for i in range(len(gth_id)) if gth_id[i]!=-1]\n",
    "        pred_seq = [config.vocab[pred_id[i]] for i in range(len(gth_id)) if gth_id[i]!=-1][:len(gth_seq)]\n",
    "        testing_output[k] = {\n",
    "            \"input\": \" \".join(input_seq),\n",
    "            \"gth\": \" \".join(gth_seq),\n",
    "            \"pred\": \" \".join(pred_seq),\n",
    "        }\n",
    "        k+=1\n",
    "    \n",
    "print(f\"\"\" {split} acc\n",
    "        | Test Loss: {round(np.mean(test_losses), 4)} \n",
    "        | Test Acc: {round(correct/demo, 4)} \n",
    "        | Test Counting Acc: {round(counting_correct/counting_demo, 4)} \n",
    "        | Test Last Acc: {round(last_correct/last_demo, 4)}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"../scripts/causal_transformer/output/{load_from_dir}/test_samples\", exist_ok=True)\n",
    "json.dump({\n",
    "        \"test_data_file\":  f\"{data_path}/{split}.txt\",\n",
    "        \"load_from\": f\"{load_from_dir}/{load_from_pt}\",\n",
    "        \"test_acc\": round(correct/demo, 4),\n",
    "        \"test_counting_acc\": round(counting_correct/counting_demo, 4),\n",
    "        \"test_last_acc\": round(last_correct/last_demo, 4),\n",
    "        \"test_loss\": round(np.mean(test_losses), 4),\n",
    "        \"testing_output\": testing_output,\n",
    "    }, \n",
    "    open(f\"../scripts/causal_transformer/output/{load_from_dir}/test_samples/{date}.json\", \"w\"), indent=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary (In the order of forward pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "task = \"counting_selective_padhelper\"\n",
    "config = eval(f\"{task}_Config()\")\n",
    "config.absolute_posemb_shift = False\n",
    "config.rotary_posemb_shift = False\n",
    "config.absolute_posemb = False\n",
    "config.rotary_posemb = False\n",
    "config.num_hidden_layers = 2\n",
    "config.embd_pdrop = 0.1\n",
    "config.attn_pdrop = 0.1\n",
    "config.resid_pdrop = 0.1\n",
    "\n",
    "model = Causal_Transformer(config)\n",
    "model = model.to(device)\n",
    "print(next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================================\n",
      "Layer (type (var_name))                       Kernel Shape     Param #          Param %\n",
      "=============================================================================================\n",
      "Causal_Transformer (Causal_Transformer)       --               --                    --\n",
      "├─Embedding (wte)                             --               55,296             0.22%\n",
      "│    └─weight                                 [1024, 54]       └─55,296\n",
      "├─Dropout (drop)                              --               --                    --\n",
      "├─ModuleList (h)                              --               --                    --\n",
      "│    └─0.ln_1.weight                          [1024]           ├─1,024\n",
      "│    └─0.ln_1.bias                            [1024]           ├─1,024\n",
      "│    └─0.attn.c_attn.weight                   [1024, 3072]     ├─3,145,728\n",
      "│    └─0.attn.c_attn.bias                     [3072]           ├─3,072\n",
      "│    └─0.attn.c_proj.weight                   [1024, 1024]     ├─1,048,576\n",
      "│    └─0.attn.c_proj.bias                     [1024]           ├─1,024\n",
      "│    └─0.ln_2.weight                          [1024]           ├─1,024\n",
      "│    └─0.ln_2.bias                            [1024]           ├─1,024\n",
      "│    └─0.mlp.c_fc.weight                      [1024, 4096]     ├─4,194,304\n",
      "│    └─0.mlp.c_fc.bias                        [4096]           ├─4,096\n",
      "│    └─0.mlp.c_proj.weight                    [4096, 1024]     ├─4,194,304\n",
      "│    └─0.mlp.c_proj.bias                      [1024]           ├─1,024\n",
      "│    └─1.ln_1.weight                          [1024]           ├─1,024\n",
      "│    └─1.ln_1.bias                            [1024]           ├─1,024\n",
      "│    └─1.attn.c_attn.weight                   [1024, 3072]     ├─3,145,728\n",
      "│    └─1.attn.c_attn.bias                     [3072]           ├─3,072\n",
      "│    └─1.attn.c_proj.weight                   [1024, 1024]     ├─1,048,576\n",
      "│    └─1.attn.c_proj.bias                     [1024]           ├─1,024\n",
      "│    └─1.ln_2.weight                          [1024]           ├─1,024\n",
      "│    └─1.ln_2.bias                            [1024]           ├─1,024\n",
      "│    └─1.mlp.c_fc.weight                      [1024, 4096]     ├─4,194,304\n",
      "│    └─1.mlp.c_fc.bias                        [4096]           ├─4,096\n",
      "│    └─1.mlp.c_proj.weight                    [4096, 1024]     ├─4,194,304\n",
      "│    └─1.mlp.c_proj.bias                      [1024]           └─1,024\n",
      "│    └─Block (0)                              --               --                    --\n",
      "│    │    └─ln_1.weight                       [1024]           ├─1,024\n",
      "│    │    └─ln_1.bias                         [1024]           ├─1,024\n",
      "│    │    └─attn.c_attn.weight                [1024, 3072]     ├─3,145,728\n",
      "│    │    └─attn.c_attn.bias                  [3072]           ├─3,072\n",
      "│    │    └─attn.c_proj.weight                [1024, 1024]     ├─1,048,576\n",
      "│    │    └─attn.c_proj.bias                  [1024]           ├─1,024\n",
      "│    │    └─ln_2.weight                       [1024]           ├─1,024\n",
      "│    │    └─ln_2.bias                         [1024]           ├─1,024\n",
      "│    │    └─mlp.c_fc.weight                   [1024, 4096]     ├─4,194,304\n",
      "│    │    └─mlp.c_fc.bias                     [4096]           ├─4,096\n",
      "│    │    └─mlp.c_proj.weight                 [4096, 1024]     ├─4,194,304\n",
      "│    │    └─mlp.c_proj.bias                   [1024]           └─1,024\n",
      "│    │    └─LayerNorm (ln_1)                  --               2,048              0.01%\n",
      "│    │    │    └─weight                       [1024]           ├─1,024\n",
      "│    │    │    └─bias                         [1024]           └─1,024\n",
      "│    │    └─Attention (attn)                  --               4,198,400         16.59%\n",
      "│    │    │    └─c_attn.weight                [1024, 3072]     ├─3,145,728\n",
      "│    │    │    └─c_attn.bias                  [3072]           ├─3,072\n",
      "│    │    │    └─c_proj.weight                [1024, 1024]     ├─1,048,576\n",
      "│    │    │    └─c_proj.bias                  [1024]           └─1,024\n",
      "│    │    └─LayerNorm (ln_2)                  --               2,048              0.01%\n",
      "│    │    │    └─weight                       [1024]           ├─1,024\n",
      "│    │    │    └─bias                         [1024]           └─1,024\n",
      "│    │    └─MLP (mlp)                         --               8,393,728         33.17%\n",
      "│    │    │    └─c_fc.weight                  [1024, 4096]     ├─4,194,304\n",
      "│    │    │    └─c_fc.bias                    [4096]           ├─4,096\n",
      "│    │    │    └─c_proj.weight                [4096, 1024]     ├─4,194,304\n",
      "│    │    │    └─c_proj.bias                  [1024]           └─1,024\n",
      "│    └─Block (1)                              --               --                    --\n",
      "│    │    └─ln_1.weight                       [1024]           ├─1,024\n",
      "│    │    └─ln_1.bias                         [1024]           ├─1,024\n",
      "│    │    └─attn.c_attn.weight                [1024, 3072]     ├─3,145,728\n",
      "│    │    └─attn.c_attn.bias                  [3072]           ├─3,072\n",
      "│    │    └─attn.c_proj.weight                [1024, 1024]     ├─1,048,576\n",
      "│    │    └─attn.c_proj.bias                  [1024]           ├─1,024\n",
      "│    │    └─ln_2.weight                       [1024]           ├─1,024\n",
      "│    │    └─ln_2.bias                         [1024]           ├─1,024\n",
      "│    │    └─mlp.c_fc.weight                   [1024, 4096]     ├─4,194,304\n",
      "│    │    └─mlp.c_fc.bias                     [4096]           ├─4,096\n",
      "│    │    └─mlp.c_proj.weight                 [4096, 1024]     ├─4,194,304\n",
      "│    │    └─mlp.c_proj.bias                   [1024]           └─1,024\n",
      "│    │    └─LayerNorm (ln_1)                  --               2,048              0.01%\n",
      "│    │    │    └─weight                       [1024]           ├─1,024\n",
      "│    │    │    └─bias                         [1024]           └─1,024\n",
      "│    │    └─Attention (attn)                  --               4,198,400         16.59%\n",
      "│    │    │    └─c_attn.weight                [1024, 3072]     ├─3,145,728\n",
      "│    │    │    └─c_attn.bias                  [3072]           ├─3,072\n",
      "│    │    │    └─c_proj.weight                [1024, 1024]     ├─1,048,576\n",
      "│    │    │    └─c_proj.bias                  [1024]           └─1,024\n",
      "│    │    └─LayerNorm (ln_2)                  --               2,048              0.01%\n",
      "│    │    │    └─weight                       [1024]           ├─1,024\n",
      "│    │    │    └─bias                         [1024]           └─1,024\n",
      "│    │    └─MLP (mlp)                         --               8,393,728         33.17%\n",
      "│    │    │    └─c_fc.weight                  [1024, 4096]     ├─4,194,304\n",
      "│    │    │    └─c_fc.bias                    [4096]           ├─4,096\n",
      "│    │    │    └─c_proj.weight                [4096, 1024]     ├─4,194,304\n",
      "│    │    │    └─c_proj.bias                  [1024]           └─1,024\n",
      "├─LayerNorm (ln_f)                            --               2,048              0.01%\n",
      "│    └─weight                                 [1024]           ├─1,024\n",
      "│    └─bias                                   [1024]           └─1,024\n",
      "├─Linear (lm_head)                            --               55,296             0.22%\n",
      "│    └─weight                                 [1024, 54]       └─55,296\n",
      "=============================================================================================\n",
      "Total params: 25,305,088\n",
      "Trainable params: 25,305,088\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 64.48\n",
      "=============================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 25.22\n",
      "Params size (MB): 101.22\n",
      "Estimated Total Size (MB): 126.44\n",
      "=============================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=============================================================================================\n",
       "Layer (type (var_name))                       Kernel Shape     Param #          Param %\n",
       "=============================================================================================\n",
       "Causal_Transformer (Causal_Transformer)       --               --                    --\n",
       "├─Embedding (wte)                             --               55,296             0.22%\n",
       "│    └─weight                                 [1024, 54]       └─55,296\n",
       "├─Dropout (drop)                              --               --                    --\n",
       "├─ModuleList (h)                              --               --                    --\n",
       "│    └─0.ln_1.weight                          [1024]           ├─1,024\n",
       "│    └─0.ln_1.bias                            [1024]           ├─1,024\n",
       "│    └─0.attn.c_attn.weight                   [1024, 3072]     ├─3,145,728\n",
       "│    └─0.attn.c_attn.bias                     [3072]           ├─3,072\n",
       "│    └─0.attn.c_proj.weight                   [1024, 1024]     ├─1,048,576\n",
       "│    └─0.attn.c_proj.bias                     [1024]           ├─1,024\n",
       "│    └─0.ln_2.weight                          [1024]           ├─1,024\n",
       "│    └─0.ln_2.bias                            [1024]           ├─1,024\n",
       "│    └─0.mlp.c_fc.weight                      [1024, 4096]     ├─4,194,304\n",
       "│    └─0.mlp.c_fc.bias                        [4096]           ├─4,096\n",
       "│    └─0.mlp.c_proj.weight                    [4096, 1024]     ├─4,194,304\n",
       "│    └─0.mlp.c_proj.bias                      [1024]           ├─1,024\n",
       "│    └─1.ln_1.weight                          [1024]           ├─1,024\n",
       "│    └─1.ln_1.bias                            [1024]           ├─1,024\n",
       "│    └─1.attn.c_attn.weight                   [1024, 3072]     ├─3,145,728\n",
       "│    └─1.attn.c_attn.bias                     [3072]           ├─3,072\n",
       "│    └─1.attn.c_proj.weight                   [1024, 1024]     ├─1,048,576\n",
       "│    └─1.attn.c_proj.bias                     [1024]           ├─1,024\n",
       "│    └─1.ln_2.weight                          [1024]           ├─1,024\n",
       "│    └─1.ln_2.bias                            [1024]           ├─1,024\n",
       "│    └─1.mlp.c_fc.weight                      [1024, 4096]     ├─4,194,304\n",
       "│    └─1.mlp.c_fc.bias                        [4096]           ├─4,096\n",
       "│    └─1.mlp.c_proj.weight                    [4096, 1024]     ├─4,194,304\n",
       "│    └─1.mlp.c_proj.bias                      [1024]           └─1,024\n",
       "│    └─Block (0)                              --               --                    --\n",
       "│    │    └─ln_1.weight                       [1024]           ├─1,024\n",
       "│    │    └─ln_1.bias                         [1024]           ├─1,024\n",
       "│    │    └─attn.c_attn.weight                [1024, 3072]     ├─3,145,728\n",
       "│    │    └─attn.c_attn.bias                  [3072]           ├─3,072\n",
       "│    │    └─attn.c_proj.weight                [1024, 1024]     ├─1,048,576\n",
       "│    │    └─attn.c_proj.bias                  [1024]           ├─1,024\n",
       "│    │    └─ln_2.weight                       [1024]           ├─1,024\n",
       "│    │    └─ln_2.bias                         [1024]           ├─1,024\n",
       "│    │    └─mlp.c_fc.weight                   [1024, 4096]     ├─4,194,304\n",
       "│    │    └─mlp.c_fc.bias                     [4096]           ├─4,096\n",
       "│    │    └─mlp.c_proj.weight                 [4096, 1024]     ├─4,194,304\n",
       "│    │    └─mlp.c_proj.bias                   [1024]           └─1,024\n",
       "│    │    └─LayerNorm (ln_1)                  --               2,048              0.01%\n",
       "│    │    │    └─weight                       [1024]           ├─1,024\n",
       "│    │    │    └─bias                         [1024]           └─1,024\n",
       "│    │    └─Attention (attn)                  --               4,198,400         16.59%\n",
       "│    │    │    └─c_attn.weight                [1024, 3072]     ├─3,145,728\n",
       "│    │    │    └─c_attn.bias                  [3072]           ├─3,072\n",
       "│    │    │    └─c_proj.weight                [1024, 1024]     ├─1,048,576\n",
       "│    │    │    └─c_proj.bias                  [1024]           └─1,024\n",
       "│    │    └─LayerNorm (ln_2)                  --               2,048              0.01%\n",
       "│    │    │    └─weight                       [1024]           ├─1,024\n",
       "│    │    │    └─bias                         [1024]           └─1,024\n",
       "│    │    └─MLP (mlp)                         --               8,393,728         33.17%\n",
       "│    │    │    └─c_fc.weight                  [1024, 4096]     ├─4,194,304\n",
       "│    │    │    └─c_fc.bias                    [4096]           ├─4,096\n",
       "│    │    │    └─c_proj.weight                [4096, 1024]     ├─4,194,304\n",
       "│    │    │    └─c_proj.bias                  [1024]           └─1,024\n",
       "│    └─Block (1)                              --               --                    --\n",
       "│    │    └─ln_1.weight                       [1024]           ├─1,024\n",
       "│    │    └─ln_1.bias                         [1024]           ├─1,024\n",
       "│    │    └─attn.c_attn.weight                [1024, 3072]     ├─3,145,728\n",
       "│    │    └─attn.c_attn.bias                  [3072]           ├─3,072\n",
       "│    │    └─attn.c_proj.weight                [1024, 1024]     ├─1,048,576\n",
       "│    │    └─attn.c_proj.bias                  [1024]           ├─1,024\n",
       "│    │    └─ln_2.weight                       [1024]           ├─1,024\n",
       "│    │    └─ln_2.bias                         [1024]           ├─1,024\n",
       "│    │    └─mlp.c_fc.weight                   [1024, 4096]     ├─4,194,304\n",
       "│    │    └─mlp.c_fc.bias                     [4096]           ├─4,096\n",
       "│    │    └─mlp.c_proj.weight                 [4096, 1024]     ├─4,194,304\n",
       "│    │    └─mlp.c_proj.bias                   [1024]           └─1,024\n",
       "│    │    └─LayerNorm (ln_1)                  --               2,048              0.01%\n",
       "│    │    │    └─weight                       [1024]           ├─1,024\n",
       "│    │    │    └─bias                         [1024]           └─1,024\n",
       "│    │    └─Attention (attn)                  --               4,198,400         16.59%\n",
       "│    │    │    └─c_attn.weight                [1024, 3072]     ├─3,145,728\n",
       "│    │    │    └─c_attn.bias                  [3072]           ├─3,072\n",
       "│    │    │    └─c_proj.weight                [1024, 1024]     ├─1,048,576\n",
       "│    │    │    └─c_proj.bias                  [1024]           └─1,024\n",
       "│    │    └─LayerNorm (ln_2)                  --               2,048              0.01%\n",
       "│    │    │    └─weight                       [1024]           ├─1,024\n",
       "│    │    │    └─bias                         [1024]           └─1,024\n",
       "│    │    └─MLP (mlp)                         --               8,393,728         33.17%\n",
       "│    │    │    └─c_fc.weight                  [1024, 4096]     ├─4,194,304\n",
       "│    │    │    └─c_fc.bias                    [4096]           ├─4,096\n",
       "│    │    │    └─c_proj.weight                [4096, 1024]     ├─4,194,304\n",
       "│    │    │    └─c_proj.bias                  [1024]           └─1,024\n",
       "├─LayerNorm (ln_f)                            --               2,048              0.01%\n",
       "│    └─weight                                 [1024]           ├─1,024\n",
       "│    └─bias                                   [1024]           └─1,024\n",
       "├─Linear (lm_head)                            --               55,296             0.22%\n",
       "│    └─weight                                 [1024, 54]       └─55,296\n",
       "=============================================================================================\n",
       "Total params: 25,305,088\n",
       "Trainable params: 25,305,088\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 64.48\n",
       "=============================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 25.22\n",
       "Params size (MB): 101.22\n",
       "Estimated Total Size (MB): 126.44\n",
       "============================================================================================="
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "References: https://github.com/TylerYep/torchinfo\n",
    "\"\"\"\n",
    "\n",
    "summary(\n",
    "    model,\n",
    "    (1, 128),\n",
    "    dtypes=[torch.long],\n",
    "    verbose=2,\n",
    "    col_width=16,\n",
    "    col_names=[\n",
    "        \"kernel_size\", \n",
    "        #\"output_size\", \n",
    "        \"num_params\", \n",
    "        \"params_percent\"\n",
    "    ],\n",
    "    row_settings=[\"var_names\"],\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([\"q\", \"b\", \"e\", \"y\", \"q\", \"m\", \"m\", \"f\", \"s\", \"k\", \"r\", \"m\", \"p\", \"b\", \"u\", \"e\", \"p\", \"y\", \"q\", \"b\", \"v\", \"j\", \"i\", \"a\", \"w\", \"e\", \"e\", \"x\", \"o\", \"s\", \"i\", \"o\", \"j\", \"e\", \"u\", \"m\", \"x\", \"n\", \"z\", \"l\", \"o\", \"f\", \"u\", \"i\", \"o\", \"k\", \"d\", \"r\", \"y\", \"s\", \"h\", \"x\", \"f\", \"a\", \"o\", \"i\", \"j\", \"r\", \"y\", \"x\", \"q\", \"b\", \"f\", \"j\", \"y\", \"p\", \"g\", \"y\", \"e\", \"a\", \"v\", \"k\", \"h\", \"l\", \"v\", \"a\", \"l\", \"b\", \"e\", \"z\", \"r\", \"b\", \"t\", \"g\", \"k\", \"u\", \"t\", \"h\", \"b\", \"q\", \"e\", \"t\", \"x\", \"f\", \"b\", \"g\", \"s\", \"p\", \"w\", \"x\", \"u\", \"q\", \"s\", \"o\", \"f\", \"c\", \"e\", \"v\", \"b\", \"l\", \"e\", \"c\", \"b\", \"b\", \"q\", \"s\", \"e\", \"q\", \"r\", \"c\", \"q\", \"x\", \"e\", \"x\", \"u\", \"z\", \"t\", \"u\", \"m\", \"w\", \"v\", \"h\", \"p\", \"h\", \"w\", \"s\", \"l\", \"q\", \"d\", \"f\", \"k\", \"c\", \"s\", \"y\", \"q\", \"w\", \"c\", \"k\", \"e\", \"l\", \"p\", \"s\", \"p\", \"f\", \"c\", \"v\", \"p\", \"r\", \"j\", \"m\", \"b\", \"e\", \"m\", \"u\", \"n\", \"a\", \"k\", \"o\", \"x\", \"x\", \"e\", \"s\", \"x\", \"y\", \"m\", \"n\", \"d\", \"u\", \"i\", \"t\", \"i\", \"n\", \"t\", \"o\", \"d\", \"v\", \"d\", \"x\", \"c\", \"r\", \"a\", \"v\", \"b\", \"g\", \"a\", \"z\", \"k\", \"m\", \"d\", \"u\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_do_math",
   "language": "python",
   "name": "llms_do_math"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
