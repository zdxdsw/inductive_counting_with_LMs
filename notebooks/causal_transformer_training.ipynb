{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, math, sys, random, re, pytz\n",
    "from datetime import datetime\n",
    "timezone = pytz.timezone('America/New_York') \n",
    "import torch\n",
    "sys.path.append(\"../scripts/\")\n",
    "from causal_transformer.model import Causal_Transformer\n",
    "from causal_transformer.config import *\n",
    "from causal_transformer.dataset import sequences_collator\n",
    "from causal_transformer.utils import get_acc\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import concatenate_datasets\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from collections import defaultdict, Counter\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "task = \"counting_samesymbol\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = eval(f\"{task}_Config()\")\n",
    "model = Causal_Transformer(config)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from 4_78127_transformer.pt\n"
     ]
    }
   ],
   "source": [
    "ckpt_dir = \"/data/yingshac/llms_do_math/scripts/causal_transformer/output\"\n",
    "load_from_dir = \"0406_134528\"\n",
    "ckpt_dir = os.path.join(ckpt_dir, load_from_dir, \"ckpts\")\n",
    "load_from_pt = sorted(os.listdir(ckpt_dir), key=lambda x: int(x.split(\"_\")[1]))[-1]\n",
    "state_dict = torch.load(os.path.join(ckpt_dir, load_from_pt), map_location=device)\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "print(f\"load from {load_from_pt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/rasp_primitives/counting_samesymbol\"\n",
    "split = \"ood_test\"\n",
    "test_data = load_dataset(\n",
    "                    \"text\", \n",
    "                    data_files={split: f\"{data_path}/{split}.txt\"})\n",
    "                \n",
    "collator = partial(sequences_collator, w2i={w:i for i,w in enumerate(config.vocab)}, max_len=config.max_position_embeddings)\n",
    "test_dataloader = DataLoader(test_data[split], shuffle=False, batch_size=config.per_device_train_batch_size, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        | Test Loss: 5.716 \n",
      "        | Test Acc: 0.75 \n",
      "        | Test Counting Acc: 0.7612 \n",
      "        | Test Last Acc: 0.0\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "counting_correct, counting_demo, last_correct, last_demo, correct, demo = 0, 0, 0, 0, 0, 0\n",
    "test_losses = []\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "model.eval()\n",
    "testing_output = {}\n",
    "\n",
    "date = datetime.now(timezone).strftime(\"%m%d_%H%M%S\")\n",
    "\n",
    "k = 0\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    logits = model(\n",
    "        batch['input_id'].to(device),\n",
    "    )\n",
    "    loss = criterion(\n",
    "        logits.view(-1, logits.size(-1)), # bs*seq_len, vocab_size\n",
    "        batch['label'].view(-1).to(device), # 1, bs*seq_len\n",
    "    )\n",
    "    test_losses.append(loss.detach().item())\n",
    "    _counting_correct, _counting_demo, _last_correct, _last_demo = get_acc(logits.detach().cpu(), batch['label'].detach().cpu(), ignore_index=-1)\n",
    "    counting_correct += _counting_correct\n",
    "    counting_demo += _counting_demo\n",
    "    last_correct += _last_correct\n",
    "    last_demo += _last_demo\n",
    "    correct += (_counting_correct + _last_correct)\n",
    "    demo += (_counting_demo + _last_demo)\n",
    "   \n",
    "    for input_id, gth_id, pred_id in zip(batch['input_id'], batch['label'], logits.argmax(dim=-1)):\n",
    "        input_seq = [config.vocab[i] for i in input_id if config.vocab[i]!='<pad>']\n",
    "        gth_seq = [config.vocab[i] for i in gth_id if i!=-1]\n",
    "        pred_seq = [config.vocab[i] for i in pred_id][:len(gth_seq)]\n",
    "        testing_output[k] = {\n",
    "            \"input\": \" \".join(input_seq),\n",
    "            \"gth\": \" \".join(gth_seq),\n",
    "            \"pred\": \" \".join(pred_seq),\n",
    "        }\n",
    "        k+=1\n",
    "    \n",
    "print(f\"\"\"\n",
    "        | Test Loss: {round(np.mean(test_losses), 4)} \n",
    "        | Test Acc: {round(correct/demo, 4)} \n",
    "        | Test Counting Acc: {round(counting_correct/counting_demo, 4)} \n",
    "        | Test Last Acc: {round(last_correct/last_demo, 4)}\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"../scripts/causal_transformer/output/{load_from_dir}/test_samples\", exist_ok=True)\n",
    "json.dump({\n",
    "        \"test_data_file\":  f\"{data_path}/{split}.txt\",\n",
    "        \"load_from\": f\"{load_from_dir}/{load_from_pt}\",\n",
    "        \"test_acc\": round(correct/demo, 4),\n",
    "        \"test_counting_acc\": round(counting_correct/counting_demo, 4),\n",
    "        \"test_last_acc\": round(last_correct/last_demo, 4),\n",
    "        \"test_loss\": round(np.mean(test_losses), 4),\n",
    "        \"testing_output\": testing_output,\n",
    "    }, \n",
    "    open(f\"../scripts/causal_transformer/output/{load_from_dir}/test_samples/{date}.json\", \"w\"), indent=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_do_math",
   "language": "python",
   "name": "llms_do_math"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
