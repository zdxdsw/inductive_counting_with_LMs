{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingshac/workspace/llms_do_math/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, random, pytz\n",
    "os.chdir(\"../scripts/s4\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tqdm.auto import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingshac/workspace/llms_do_math/scripts/s4/model.py:51: UserWarning: CUDA extension for structured kernels (Cauchy and Vandermonde multiplication) not found. Install by going to extensions/kernels/ and running `python setup.py install`, for improved speed and memory efficiency. Note that the kernel changed for state-spaces 4.0 and must be recompiled.\n",
      "  warnings.warn(\n",
      "/home/yingshac/workspace/llms_do_math/scripts/s4/model.py:149: UserWarning: Falling back on slow Cauchy and Vandermonde kernel. Install at least one of pykeops or the CUDA extension for better speed and memory efficiency.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from model import S4Model  # Can use full version instead of minimal S4D standalone below\n",
    "from config_taskspecific import *\n",
    "from config import *\n",
    "sys.path.append(\"../\")\n",
    "from causal_transformer.utils import trim_task\n",
    "from causal_transformer.dataset import sequences_collator\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "task = \"counting_samesymbol_shiftedstart3\"\n",
    "config = eval(f\"{task}_Config()\")\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingshac/workspace/llms_do_math/venv/lib/python3.8/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing S4 (H, N, L) = (128, 32, None)\n",
      "Constructing S4 (H, N, L) = (128, 32, None)\n",
      "Constructing S4 (H, N, L) = (128, 32, None)\n",
      "Constructing S4 (H, N, L) = (128, 32, None)\n"
     ]
    }
   ],
   "source": [
    "model = S4Model(config)\n",
    "model = model.to(device)\n",
    "if device == 'cuda': cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [trim_task(task)] + config.aux_tasks\n",
    "\n",
    "train_data = concatenate_datasets(\n",
    "                    [load_dataset(\n",
    "                            \"text\", \n",
    "                            data_files={\"train\": f\"{config.train_data_path}/{task}/train.txt\"}\n",
    "                            )['train'] for task in tasks]\n",
    "                )\n",
    "val_data = load_dataset(\n",
    "                    \"text\", \n",
    "                    data_files={\"validation\": f\"{config.eval_data_path}/{trim_task(task)}/val.txt\"}\n",
    "                    )['validation']\n",
    "\n",
    "# args.max_seen_len = max([len([x for x in json.loads(l['text'])[0] if x != \"<pad>\"]) for l in val_data])\n",
    "# Print(f\"max_seen_len for {args.task} = {args.max_seen_len}\")\n",
    "\n",
    "collator = partial(sequences_collator, \n",
    "                    w2i={w:i for i,w in enumerate(config.vocab)}, \n",
    "                    max_seq_len=config.max_seq_len,\n",
    "                    max_position_embeddings=config.max_seq_len,\n",
    "                    augmentation=None,\n",
    "                )\n",
    "\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=config.per_device_train_batch_size, collate_fn=collator)\n",
    "val_dataloader = DataLoader(val_data, shuffle=False, batch_size=config.per_device_eval_batch_size, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_optimizer(model, lr, weight_decay, epochs):\n",
    "    \"\"\"\n",
    "    S4 requires a specific optimizer setup.\n",
    "\n",
    "    The S4 layer (A, B, C, dt) parameters typically\n",
    "    require a smaller learning rate (typically 0.001), with no weight decay.\n",
    "\n",
    "    The rest of the model can be trained with a higher learning rate (e.g. 0.004, 0.01)\n",
    "    and weight decay (if desired).\n",
    "    \"\"\"\n",
    "\n",
    "    # All parameters in the model\n",
    "    all_parameters = list(model.parameters())\n",
    "\n",
    "    # General parameters don't contain the special _optim key\n",
    "    params = [p for p in all_parameters if not hasattr(p, \"_optim\")]\n",
    "\n",
    "    # Create an optimizer with the general parameters\n",
    "    optimizer = optim.AdamW(params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Add parameters with special hyperparameters\n",
    "    hps = [getattr(p, \"_optim\") for p in all_parameters if hasattr(p, \"_optim\")]\n",
    "    hps = [\n",
    "        dict(s) for s in sorted(list(dict.fromkeys(frozenset(hp.items()) for hp in hps)))\n",
    "    ]  # Unique dicts\n",
    "    for hp in hps:\n",
    "        params = [p for p in all_parameters if getattr(p, \"_optim\", None) == hp]\n",
    "        optimizer.add_param_group(\n",
    "            {\"params\": params, **hp}\n",
    "        )\n",
    "\n",
    "    # Create a lr scheduler\n",
    "    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=patience, factor=0.2)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
    "\n",
    "    # Print optimizer info\n",
    "    keys = sorted(set([k for hp in hps for k in hp.keys()]))\n",
    "    for i, g in enumerate(optimizer.param_groups):\n",
    "        group_hps = {k: g.get(k, None) for k in keys}\n",
    "        print(' | '.join([\n",
    "            f\"Optimizer group {i}\",\n",
    "            f\"{len(g['params'])} tensors\",\n",
    "        ] + [f\"{k} {v}\" for k, v in group_hps.items()]))\n",
    "\n",
    "    return optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer group 0 | 22 tensors | lr 0.01 | weight_decay 0.01\n",
      "Optimizer group 1 | 4 tensors | lr 0.001 | weight_decay 0.01\n",
      "Optimizer group 2 | 20 tensors | lr 0.001 | weight_decay 0.0\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer, scheduler = setup_optimizer(\n",
    "    model, lr=config.learning_rate, weight_decay=config.weight_decay, epochs=config.num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "def train():\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pbar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "    for batch_idx, batch in pbar:\n",
    "        inputs, targets = batch['input_id'], batch['label']\n",
    "        inputs, targets = inputs.to(device), targets.to(device).view(-1)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs) # B, seq_len, vocab_size\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        loss = criterion(\n",
    "            logits, # bs*seq_len, vocab_size\n",
    "            targets,\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = logits.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        pbar.set_description(\n",
    "            'Batch Idx: (%d/%d) | Loss: %.3f | Acc: %.3f%% (%d/%d)' %\n",
    "            (batch_idx, len(train_dataloader), train_loss/(batch_idx+1), 100.*correct/total, correct, total)\n",
    "        )\n",
    "\n",
    "def eval(epoch, dataloader, checkpoint=False):\n",
    "    global best_acc\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "        for batch_idx, batch in pbar:\n",
    "            inputs, targets = batch['input_id'], batch['label']\n",
    "            inputs, targets = inputs.to(device), targets.to(device).view(-1)\n",
    "            logits = model(inputs) # B, seq_len, vocab_size\n",
    "            logits = logits.view(-1, logits.size(-1))\n",
    "            loss = criterion(\n",
    "                logits, # bs*seq_len, vocab_size\n",
    "                targets,\n",
    "            )\n",
    "\n",
    "            eval_loss += loss.item()\n",
    "            _, predicted = logits.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            pbar.set_description(\n",
    "                'Batch Idx: (%d/%d) | Loss: %.3f | Acc: %.3f%% (%d/%d)' %\n",
    "                (batch_idx, len(dataloader), eval_loss/(batch_idx+1), 100.*correct/total, correct, total)\n",
    "            )\n",
    "\n",
    "    # Save checkpoint.\n",
    "    if checkpoint:\n",
    "        acc = 100.*correct/total\n",
    "        if acc > best_acc:\n",
    "            state = {\n",
    "                'model': model.state_dict(),\n",
    "                'acc': acc,\n",
    "                'epoch': epoch,\n",
    "            }\n",
    "            if not os.path.isdir('checkpoint'):\n",
    "                os.mkdir('checkpoint')\n",
    "            torch.save(state, './checkpoint/ckpt.pth')\n",
    "            best_acc = acc\n",
    "\n",
    "        return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S4: Initializing kernel to length 128\n",
      "S4: Initializing kernel to length 128\n",
      "S4: Initializing kernel to length 128\n",
      "S4: Initializing kernel to length 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Idx: (31249/31250) | Loss: 0.011 | Acc: 38.944% (49848070/128000000): 100%|██████████| 31250/31250 [11:40<00:00, 44.60it/s]\n",
      "Batch Idx: (0/1) | Loss: 0.000 | Acc: 39.062% (2500/6400): 100%|██████████| 1/1 [00:00<00:00, 60.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Idx: (9856/31250) | Loss: 0.000 | Acc: 39.062% (15771185/40374272):  32%|███▏      | 9857/31250 [03:45<08:09, 43.73it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# if epoch == 0:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#     pbar.set_description('Epoch: %d' % (epoch))\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#     pbar.set_description('Epoch: %d | Val acc: %1.3f' % (epoch, val_acc))\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(epoch, val_dataloader, checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#eval(epoch, testloader)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m     16\u001b[0m     logits, \u001b[38;5;66;03m# bs*seq_len, vocab_size\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     targets,\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/workspace/llms_do_math/venv/lib/python3.8/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/llms_do_math/venv/lib/python3.8/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/llms_do_math/venv/lib/python3.8/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#pbar = tqdm(range(start_epoch, config.num_epochs))\n",
    "for epoch in start_epoch, config.num_epochs:\n",
    "    print(f\"start epoch {epoch}\")\n",
    "    # if epoch == 0:\n",
    "    #     pbar.set_description('Epoch: %d' % (epoch))\n",
    "    # else:\n",
    "    #     pbar.set_description('Epoch: %d | Val acc: %1.3f' % (epoch, val_acc))\n",
    "    train()\n",
    "    val_acc = eval(epoch, val_dataloader, checkpoint=True)\n",
    "    #eval(epoch, testloader)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_do_math",
   "language": "python",
   "name": "llms_do_math"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
