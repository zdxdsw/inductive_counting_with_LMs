{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, random, io, pytz, argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from peft import (\n",
    "        get_peft_model, \n",
    "        prepare_model_for_kbit_training, \n",
    "        LoraConfig, \n",
    "        PeftModel,\n",
    "        AutoPeftModelForCausalLM\n",
    "    )\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_path = \"../data/feed_decoder_LM/regular/len/q_prompt.txt\"\n",
    "with open(prompt_path, \"r\") as f: prompt_template = \"\\n\".join(f.readlines()).strip()\n",
    "def preprocess(prompt_template, input_str, answer=None, eos_token=\"</s>\"):\n",
    "  prompt = prompt_template.format(input_str)\n",
    "  response = f\"{str(answer) + '.' + eos_token if answer else ''} \"\n",
    "  text = \"### Question: {}\\n ### Answer: {}\".format(prompt, response) #(\" \").join([prompt, response])\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.01s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\" #\"/home/yingshan/data/LLM/vicuna-13b\" #\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "\n",
    "lora_adapter = \"../scripts/llama/output/1114_190018/ckpts/checkpoint-7880\"\n",
    "peft_model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    #model, \n",
    "    lora_adapter, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map={\"\": 0}\n",
    "    #offload_folder=\"lora_results/lora_7/temp\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(lora_adapter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32001, 4096)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "tokenizer.padding_side = 'right'\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 3, 'input_str': '602'}\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"../data/finetune/len/uniform_split\", split=\"validation\")\n",
    "pprint(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: What is length of the string \"5169\"?\n",
      " ### Answer: 4.</s> \n"
     ]
    }
   ],
   "source": [
    "print(preprocess(prompt_template, data[140][\"input_str\"], data[140][\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/output_decoder_LM/llama2-7b/len/zeroshot/val.jsonl\", \"w\") as f:\n",
    "  for batch in tqdm(dataloader):\n",
    "    input_text = [preprocess(prompt_template, s) for s in batch['input_str']]\n",
    "    tokenized_text = tokenizer(input_text, \n",
    "                             padding = 'longest',\n",
    "                             max_length = 512,\n",
    "                             truncation = True, \n",
    "                             return_tensors=\"pt\"\n",
    "                            )\n",
    "    input_tokens = tokenized_text[\"input_ids\"].to(\"cuda\")\n",
    "    attn_mask = tokenized_text[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "    with torch.cuda.amp.autocast():\n",
    "      generation_output = model.generate(\n",
    "          input_ids=input_tokens,\n",
    "          attention_mask = attn_mask,\n",
    "          max_new_tokens=10,\n",
    "          do_sample=False,\n",
    "          temperature=1.0,\n",
    "          top_k=1,\n",
    "          num_return_sequences=1,\n",
    "          eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    for s, a, o in zip(batch['input_str'], batch['answer'], generation_output):\n",
    "      op = tokenizer.decode(o, skip_special_tokens=True)\n",
    "      f.write(json.dumps([s, a.item(), op]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question: What is length of the string \"0572674832\"?\n",
      " ### Answer:  \n",
      "```\n",
      "12\n",
      "```\n",
      "\n",
      "##\n"
     ]
    }
   ],
   "source": [
    "input_text = preprocess(prompt_template, data[790][\"input_str\"])\n",
    "input_tokens = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "  generation_output = model.generate(\n",
    "      input_ids=input_tokens,\n",
    "      max_new_tokens=10,\n",
    "      do_sample=False,\n",
    "      #do_sample=True,\n",
    "      #top_k=None,\n",
    "      #top_p=0.9,\n",
    "      #temperature=0,\n",
    "      #repetition_penalty=1.15,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in peft_model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingshan/llms_do_math/venv/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/yingshan/llms_do_math/venv/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> ### Question: What is length of the string \"0572674832\"?\n",
      " ### Answer:  38.</s>\n"
     ]
    }
   ],
   "source": [
    "input_text = preprocess(prompt_template, data[790][\"input_str\"])\n",
    "input_tokens = tokenizer(input_text, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "  generation_output = peft_model.generate(\n",
    "      input_ids=input_tokens,\n",
    "      max_new_tokens=5,\n",
    "      do_sample=False,\n",
    "      #top_k=10,\n",
    "      #top_p=0.1,\n",
    "      #temperature=0.3,\n",
    "      #repetition_penalty=1.15,\n",
    "      #num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"070193108739725\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check: It's indeed deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "output_dir = \"../data/output_decoder_LM/llama2-7b/len/zeroshot/\"\n",
    "ANS = []\n",
    "for f in os.listdir(output_dir):\n",
    "    if \"1113_\" in f:\n",
    "        ANS.append([])\n",
    "        lines = open(os.path.join(output_dir, f), \"r\").readlines()\n",
    "        for l in lines:\n",
    "            ANS[-1].append(json.loads(l)[-1].split(\"\\n ### Answer:\")[-1].strip())\n",
    "print(len(ANS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in zip(ANS[1], ANS[2]):\n",
    "    if not i == j:\n",
    "        print(i)\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data --- LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, json, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_num_digit = 1000\n",
    "data = {}\n",
    "for num_digit in trange(129, 257):\n",
    "    if num_digit <= 6:\n",
    "        strings = random.sample([str(x) for x in range(10**num_digit)], samples_per_num_digit)\n",
    "        strings = [\"0\"*(num_digit-len(s)) + s for s in strings]\n",
    "        \n",
    "    else:\n",
    "        strings = set()\n",
    "        while len(strings) < samples_per_num_digit:\n",
    "            s = \"\".join([random.choice(\"1234567890\") for i in range(num_digit)])\n",
    "            strings.add(s)\n",
    "    data[num_digit] = list(strings)\n",
    "json.dump(data, open(\"../data/finetune/len/finetune_129_256.json\", \"w\"), indent=2)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples_per_num_digit = 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:06<00:00, 19.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12600 12600\n"
     ]
    }
   ],
   "source": [
    "# Uniform split\n",
    "samples_per_num_digit_train, samples_per_num_digit_val = 100, 100\n",
    "\n",
    "data = json.load(open(\"../data/finetune/len/finetune.json\", \"r\"))\n",
    "print(\"samples_per_num_digit = {}\".format(len(data[list(data.keys())[0]])))\n",
    "\n",
    "train = pd.DataFrame(columns=[\"input_str\", \"answer\"])\n",
    "val = pd.DataFrame(columns=[\"input_str\", \"answer\"])\n",
    "for num_digit in tqdm(data):\n",
    "    for s in data[num_digit][:samples_per_num_digit_train]:\n",
    "        row = {\n",
    "            \"input_str\": [s],\n",
    "            \"answer\": [num_digit]\n",
    "        }\n",
    "        row = pd.DataFrame(row)\n",
    "        train = pd.concat([train, pd.DataFrame(row)])\n",
    "    for s in data[num_digit][\n",
    "        samples_per_num_digit_train:samples_per_num_digit_train+samples_per_num_digit_val\n",
    "    ]:\n",
    "        row = {\n",
    "            \"input_str\": [s],\n",
    "            \"answer\": [num_digit]\n",
    "        }\n",
    "        row = pd.DataFrame(row)\n",
    "        val = pd.concat([val, pd.DataFrame(row)])\n",
    "print(len(train), len(val))\n",
    "\n",
    "train.to_csv(\"../data/finetune/len/uniform_split/train.csv\", index=False)\n",
    "val.to_csv(\"../data/finetune/len/uniform_split/val.csv\", index=False)\n",
    "\n",
    "#val.to_csv(\"../data/finetune/len/uniform_large_split/val.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples_per_num_digit = 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:06<00:00, 18.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12600 12600\n"
     ]
    }
   ],
   "source": [
    "# odd_even_3:1 split\n",
    "samples_per_num_digit_val = 100\n",
    "\n",
    "data = json.load(open(\"../data/finetune/len/finetune.json\", \"r\"))\n",
    "print(\"samples_per_num_digit = {}\".format(len(data[list(data.keys())[0]])))\n",
    "\n",
    "train = pd.DataFrame(columns=[\"input_str\", \"answer\"])\n",
    "val = pd.DataFrame(columns=[\"input_str\", \"answer\"])\n",
    "for num_digit in tqdm(data):\n",
    "    if int(num_digit)%2 == 1: samples_per_num_digit_train = 150\n",
    "    else: samples_per_num_digit_train = 50\n",
    "    for s in data[num_digit][:samples_per_num_digit_train]:\n",
    "        row = {\n",
    "            \"input_str\": [s],\n",
    "            \"answer\": [num_digit]\n",
    "        }\n",
    "        row = pd.DataFrame(row)\n",
    "        train = pd.concat([train, pd.DataFrame(row)])\n",
    "    for s in data[num_digit][\n",
    "        samples_per_num_digit_train:samples_per_num_digit_train+samples_per_num_digit_val\n",
    "    ]:\n",
    "        row = {\n",
    "            \"input_str\": [s],\n",
    "            \"answer\": [num_digit]\n",
    "        }\n",
    "        row = pd.DataFrame(row)\n",
    "        val = pd.concat([val, pd.DataFrame(row)])\n",
    "print(len(train), len(val))\n",
    "\n",
    "train.to_csv(\"../data/finetune/len/odd_even_3:1_split/train.csv\", index=False)\n",
    "val.to_csv(\"../data/finetune/len/odd_even_3:1_split/val.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples_per_num_digit = 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:06<00:00, 18.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12600 12600\n"
     ]
    }
   ],
   "source": [
    "# odd_even_9:1 split\n",
    "samples_per_num_digit_val = 100\n",
    "\n",
    "data = json.load(open(\"../data/finetune/len/finetune.json\", \"r\"))\n",
    "print(\"samples_per_num_digit = {}\".format(len(data[list(data.keys())[0]])))\n",
    "\n",
    "train = pd.DataFrame(columns=[\"input_str\", \"answer\"])\n",
    "val = pd.DataFrame(columns=[\"input_str\", \"answer\"])\n",
    "for num_digit in tqdm(data):\n",
    "    if int(num_digit)%2 == 1: samples_per_num_digit_train = 180\n",
    "    else: samples_per_num_digit_train = 20\n",
    "    for s in data[num_digit][:samples_per_num_digit_train]:\n",
    "        row = {\n",
    "            \"input_str\": [s],\n",
    "            \"answer\": [num_digit]\n",
    "        }\n",
    "        row = pd.DataFrame(row)\n",
    "        train = pd.concat([train, pd.DataFrame(row)])\n",
    "    for s in data[num_digit][\n",
    "        samples_per_num_digit_train:samples_per_num_digit_train+samples_per_num_digit_val\n",
    "    ]:\n",
    "        row = {\n",
    "            \"input_str\": [s],\n",
    "            \"answer\": [num_digit]\n",
    "        }\n",
    "        row = pd.DataFrame(row)\n",
    "        val = pd.concat([val, pd.DataFrame(row)])\n",
    "print(len(train), len(val))\n",
    "\n",
    "train.to_csv(\"../data/finetune/len/odd_even_9:1_split/train.csv\", index=False)\n",
    "val.to_csv(\"../data/finetune/len/odd_even_9:1_split/val.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# odd_only split\n",
    "samples_per_num_digit_val = 100\n",
    "\n",
    "data = json.load(open(\"../data/finetune/len/finetune.json\", \"r\"))\n",
    "print(\"samples_per_num_digit = {}\".format(len(data[list(data.keys())[0]])))\n",
    "\n",
    "train = pd.DataFrame(columns=[\"input_str\", \"answer\"])\n",
    "val = pd.DataFrame(columns=[\"input_str\", \"answer\"])\n",
    "for num_digit in tqdm(data):\n",
    "    if int(num_digit)%2 == 1: samples_per_num_digit_train = 100\n",
    "    else: samples_per_num_digit_train = 0\n",
    "    for s in data[num_digit][:samples_per_num_digit_train]:\n",
    "        row = {\n",
    "            \"input_str\": [s],\n",
    "            \"answer\": [num_digit]\n",
    "        }\n",
    "        row = pd.DataFrame(row)\n",
    "        train = pd.concat([train, pd.DataFrame(row)])\n",
    "    for s in data[num_digit][\n",
    "        samples_per_num_digit_train:samples_per_num_digit_train+samples_per_num_digit_val\n",
    "    ]:\n",
    "        row = {\n",
    "            \"input_str\": [s],\n",
    "            \"answer\": [num_digit]\n",
    "        }\n",
    "        row = pd.DataFrame(row)\n",
    "        val = pd.concat([val, pd.DataFrame(row)])\n",
    "print(len(train), len(val))\n",
    "\n",
    "train.to_csv(\"../data/finetune/len/odd_only_split/train.csv\", index=False)\n",
    "val.to_csv(\"../data/finetune/len/odd_only_split/val.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples_per_num_digit = 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/126 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:03<00:00, 35.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 12600\n"
     ]
    }
   ],
   "source": [
    "# length % k = 3 split\n",
    "k = 20\n",
    "samples_per_num_digit_val = 100\n",
    "\n",
    "data = json.load(open(\"../data/finetune/len/finetune.json\", \"r\"))\n",
    "print(\"samples_per_num_digit = {}\".format(len(data[list(data.keys())[0]])))\n",
    "\n",
    "train = pd.DataFrame(columns=[\"input_str\", \"answer\"])\n",
    "val = pd.DataFrame(columns=[\"input_str\", \"answer\"])\n",
    "for num_digit in tqdm(data):\n",
    "    if int(num_digit)%k == 3: samples_per_num_digit_train = 100\n",
    "    else: samples_per_num_digit_train = 0\n",
    "    for s in data[num_digit][:samples_per_num_digit_train]:\n",
    "        row = {\n",
    "            \"input_str\": [s],\n",
    "            \"answer\": [num_digit]\n",
    "        }\n",
    "        row = pd.DataFrame(row)\n",
    "        train = pd.concat([train, pd.DataFrame(row)])\n",
    "    for s in data[num_digit][\n",
    "        samples_per_num_digit_train:samples_per_num_digit_train+samples_per_num_digit_val\n",
    "    ]:\n",
    "        row = {\n",
    "            \"input_str\": [s],\n",
    "            \"answer\": [num_digit]\n",
    "        }\n",
    "        row = pd.DataFrame(row)\n",
    "        val = pd.concat([val, pd.DataFrame(row)])\n",
    "print(len(train), len(val))\n",
    "\n",
    "train.to_csv(f\"../data/finetune/len/length_mod_{k}=3_split/train.csv\", index=False)\n",
    "val.to_csv(f\"../data/finetune/len/length_mod_{k}=3_split/val.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json, re, math\n",
    "from pprint import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_dir': '../../data/finetune/len/length_extrapolation',\n",
      " 'load_from_ckpt': 'checkpoint-440'}\n",
      "testing acc = 0.0\n"
     ]
    }
   ],
   "source": [
    "with open(\"../scripts/llama/output/1205_105919/test_samples/1205_113333.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "header, lines = json.loads(lines[0]), lines[1:]\n",
    "pprint(header)\n",
    "em = 0\n",
    "all_preds = []\n",
    "for l in lines:\n",
    "    gth, pred = json.loads(l)\n",
    "    gth_len = int(gth.split(\".\")[0])\n",
    "    pred_len = -1\n",
    "    find = re.findall(r'(\\d+)', pred)\n",
    "    if find: pred_len = int(find[0])\n",
    "    all_preds.append(pred_len)\n",
    "    em += int(gth_len == pred_len)\n",
    "print(f\"testing acc = {em / len(lines)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(123, 6767), (153, 4108), (103, 969), (158, 333), (122, 129), (128, 110), (1533, 76), (108, 59), (120, 58), (163, 37)]\n",
      "[123, 153, 103, 158, 122, 128, 1533, 108, 120, 163]\n"
     ]
    }
   ],
   "source": [
    "most_common = Counter(all_preds).most_common(10)\n",
    "print(most_common)\n",
    "print([x[0] for x in most_common])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing acc = 0.06285714285714286\n",
      "avg abs_error =  10.661269841269842\n"
     ]
    }
   ],
   "source": [
    "with open(\"../scripts/llama/output/1205_105919/eval_samples/1205_111535.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "em = 0\n",
    "abs_errors = []\n",
    "all_preds = []\n",
    "for l in lines:\n",
    "    gth, pred = json.loads(l)\n",
    "    gth_len = int(gth.split(\".\")[0])\n",
    "    pred_len = -1\n",
    "    find = re.findall(r'(\\d+)', pred)\n",
    "    if find: pred_len = int(find[0])\n",
    "    all_preds.append(pred_len)\n",
    "    em += int(gth_len == pred_len) # int(-10<=gth_len-pred_len<=10) #\n",
    "    abs_errors.append(np.abs(gth_len - pred_len))\n",
    "print(f\"testing acc = {em / len(lines)}\") \n",
    "print(f\"avg abs_error = \", np.mean(abs_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
